---
title: "Marine biodiversity data ingestion for OBIS (DwC translation)"
subtitle: "A fine-tuned global distribution dataset of marine forests"
author:
- [your name here]
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
---

This document describes how we map the checklist data to Darwin Core. The source file for this document can be found [here](https://github.com/iobis/mpaeu_di_marine_forests/blob/master/src/obisdi_general.Rmd).

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Load libraries:

```{r}
library(tidyverse)      # Data manipulation
library(data.table)     # Data reading
library(obisdi)         # Tools for data ingestion for OBIS
library(here)           # Get paths (important!)
library(arrow)          # To deal with parquet files
```

# Read source data

The checklist will be downloaded from FigShare. We use the `obisdi` function to do the download and also to obtain metadata. Because the files are large, we added a line to control and only download the data once and save the resulting metadata:

```{r}
# Get the path to data/raw
raw_path <- here("data", "raw")

# See if files were already downloaded
lf <- list.files(raw_path)
if (!any(grepl("figshare", lf))) {
  fig_details <- get_figshare(article_id = 7854767, download_files = T,
                              save_meta = T, path = raw_path)
}
```
Following the download the details of the dataset can be accessed from the file `r paste0("data/raw/", list.files(raw_path)[grepl("figshare", list.files(raw_path))])`.

```{r include=FALSE}
meta <- read.csv(list.files(raw_path, full.names = T)[grepl("figshare", list.files(raw_path))])
```


Title: `r meta$title`  
Authors: `r meta$authors`  
Date (dmy format): `r format(as.Date(meta$date), "%d/%m/%Y")`  
DOI: `r meta$doi`  
URL: `r meta$url`  

# Preprocessing

First we reduce the size of the raw files by converting them to the `parquet` format. We keep only the flagged file which is the one that we will include in the OBIS database.

```{r results='hide'}
raw_files <- list.files(raw_path, full.names = T)
file.remove(raw_files[-grep("databaseAll.csv|databaseAll.parquet|metadata", raw_files)])

# We just run the conversion in the first knitting of this document
if (any(grepl("databaseAll.csv", raw_files))) {
  flagged <- fread(paste0(raw_path, "/databaseAll.csv"))
  write_parquet(flagged, paste0(raw_path, "/databaseAll.parquet"))
  rm(flagged)
  file.remove(paste0(raw_path, "/databaseAll.csv"))
}
```

Now we can load the parquet file containing the dataset we will work with.

```{r}
dataset <- read_parquet(paste0(raw_path, "/databaseAll.parquet"))
head(dataset)
```

We will filter the dataset to remove those records that are already available on OBIS. In that case, we will filter by "Ocean Biogeographic Information System" (old name) and "Ocean Biodiversity Information System".

```{r}
dataset_filt <- dataset %>%
  mutate(proc_bibliographicCitation = tolower(bibliographicCitation)) %>%
  filter(!grepl("ocean biogeographic information system|ocean biodiversity information system", proc_bibliographicCitation)) %>%
  select(-proc_bibliographicCitation)
```

# Darwin Core mapping

This dataset is already on the DwC standard, so no mapping will be necessary. However, we need to separate the flags into a new table, what will contain the `MeasurementOrFacts`:

```{r}
flags <- dataset_filt %>%
  select(id, starts_with("flag"))
```

Now we convert the `flags` object to the right format:

```{r}
flags_conv <- flags %>%
  pivot_longer(cols = 2:4,
               names_to = "measurementType",
               values_to = "measurementValue") %>%
  mutate(measurementValue = as.numeric(measurementValue))
```

We can check the conversion worked by tabulating the values:

```{r}
cbind(data.frame(table(flags$flagHumanCuratedDistribution)),
               Freq_conv = data.frame(table(
                 flags_conv$measurementValue[flags_conv$measurementType == "flagHumanCuratedDistribution"]
               ))[,2])

cbind(data.frame(table(flags$flagMachineOnLand)),
               Freq_conv = data.frame(table(
                 flags_conv$measurementValue[flags_conv$measurementType == "flagMachineOnLand"]
               ))[,2])

cbind(data.frame(table(flags$flagMachineSuitableLightBottom)),
               Freq_conv = data.frame(table(
                 flags_conv$measurementValue[flags_conv$measurementType == "flagMachineSuitableLightBottom"]
               ))[,2])
```

That's all we needed to do with the data for now.

# Post-processing

As a final step, we just remove the `MeasurementOrFact` column of the other object, as this will be supplied to the IPT in a different file. 

```{r}
dataset_filt <- dataset_filt %>%
  select(-starts_with("flag"))
```

And those are the final objects:

```{r}
dataset_filt

flags_conv
```

# Export final files

We can then save the final files:

```{r}
processed_path <- here("data", "processed")

write_csv(flags_conv, paste0(processed_path, "/extension.csv.gz"))

write_csv(dataset_filt, paste0(processed_path, "/occurrences.csv.gz"))
```

And we check if the files are saved:

```{r}
list.files(processed_path)
```


